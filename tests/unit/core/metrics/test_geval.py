# pylint: disable=too-many-public-methods,protected-access

"""Tests for GEval metrics handler."""
from typing import Any

import pytest
from pytest_mock import MockerFixture
from deepeval.metrics.g_eval import Rubric
from deepeval.test_case import LLMTestCaseParams

from lightspeed_evaluation.core.metrics.geval import GEvalHandler
from lightspeed_evaluation.core.metrics.manager import MetricLevel


class TestGEvalHandler:
    """Test cases for GEvalHandler class."""

    @pytest.fixture
    def mock_llm_manager(self, mocker: MockerFixture) -> Any:
        """Create a mock DeepEvalLLMManager."""
        mock_manager = mocker.MagicMock()
        mock_llm = mocker.MagicMock()
        mock_manager.get_llm.return_value = mock_llm
        return mock_manager

    @pytest.fixture
    def mock_metric_manager(self, mocker: MockerFixture) -> Any:
        """Create a mock MetricManager."""
        return mocker.MagicMock()

    @pytest.fixture
    def handler(self, mock_llm_manager: Any, mock_metric_manager: Any) -> GEvalHandler:
        """Create a GEvalHandler instance with mocked dependencies."""
        return GEvalHandler(
            deepeval_llm_manager=mock_llm_manager,
            metric_manager=mock_metric_manager,
        )

    def test_initialization(
        self, mock_llm_manager: Any, mock_metric_manager: Any
    ) -> None:
        """Test GEvalHandler initialization with required dependencies."""
        handler = GEvalHandler(
            deepeval_llm_manager=mock_llm_manager,
            metric_manager=mock_metric_manager,
        )

        assert handler.deepeval_llm_manager == mock_llm_manager
        assert handler.metric_manager == mock_metric_manager

    def test_convert_evaluation_params_field_names(self, handler: GEvalHandler) -> None:
        """Test conversion of evaluation data field names to LLMTestCaseParams enum."""
        params = ["query", "response", "expected_response"]
        result = handler._convert_evaluation_params(params)

        assert result is not None
        assert len(result) == 3
        assert LLMTestCaseParams.INPUT in result
        assert LLMTestCaseParams.ACTUAL_OUTPUT in result
        assert LLMTestCaseParams.EXPECTED_OUTPUT in result

    def test_convert_evaluation_params_with_contexts(
        self, handler: GEvalHandler
    ) -> None:
        """Test conversion including contexts and retrieval_context fields."""
        params = ["query", "response", "contexts", "retrieval_context"]
        result = handler._convert_evaluation_params(params)

        assert result is not None
        assert len(result) == 4
        assert LLMTestCaseParams.INPUT in result
        assert LLMTestCaseParams.ACTUAL_OUTPUT in result
        assert LLMTestCaseParams.CONTEXT in result
        assert LLMTestCaseParams.RETRIEVAL_CONTEXT in result

    def test_convert_evaluation_params_enum_values_backward_compat(
        self, handler: GEvalHandler
    ) -> None:
        """Test conversion with direct enum value strings (backward compatibility)."""
        params = ["INPUT", "ACTUAL_OUTPUT", "EXPECTED_OUTPUT"]
        result = handler._convert_evaluation_params(params)

        assert result is not None
        assert len(result) == 3
        assert LLMTestCaseParams.INPUT in result
        assert LLMTestCaseParams.ACTUAL_OUTPUT in result
        assert LLMTestCaseParams.EXPECTED_OUTPUT in result

    def test_convert_evaluation_params_invalid_returns_none(
        self, handler: GEvalHandler
    ) -> None:
        """Test that invalid params return None to allow GEval auto-detection."""
        params = ["invalid_param", "another_invalid"]
        result = handler._convert_evaluation_params(params)

        assert result is None

    def test_convert_evaluation_params_empty_returns_none(
        self, handler: GEvalHandler
    ) -> None:
        """Test that empty params list returns None."""
        result = handler._convert_evaluation_params([])
        assert result is None

    def test_convert_evaluation_params_mixed_invalid_returns_none(
        self, handler: GEvalHandler
    ) -> None:
        """Test that any invalid param causes None return."""
        params = ["query", "invalid_param", "response"]
        result = handler._convert_evaluation_params(params)

        # Should return None because of the invalid param
        assert result is None

    def test_get_geval_config_uses_metric_manager(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that _get_geval_config delegates to MetricManager."""
        expected_config = {
            "criteria": "Test criteria",
            "evaluation_params": ["query", "response"],
            "threshold": 0.8,
        }
        mock_metric_manager.get_metric_metadata.return_value = expected_config

        conv_data = mocker.MagicMock()
        config = handler._get_geval_config(
            metric_name="test_metric",
            conv_data=conv_data,
            turn_data=None,
            is_conversation=True,
        )

        assert config == expected_config
        mock_metric_manager.get_metric_metadata.assert_called_once_with(
            metric_identifier="geval:test_metric",
            level=MetricLevel.CONVERSATION,
            conv_data=conv_data,
            turn_data=None,
        )

    def test_get_geval_config_turn_level(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test retrieving turn-level config uses correct MetricLevel."""
        expected_config = {"criteria": "Turn criteria", "threshold": 0.9}
        mock_metric_manager.get_metric_metadata.return_value = expected_config

        conv_data = mocker.MagicMock()
        turn_data = mocker.MagicMock()

        config = handler._get_geval_config(
            metric_name="turn_metric",
            conv_data=conv_data,
            turn_data=turn_data,
            is_conversation=False,
        )

        assert config == expected_config
        mock_metric_manager.get_metric_metadata.assert_called_once_with(
            metric_identifier="geval:turn_metric",
            level=MetricLevel.TURN,
            conv_data=conv_data,
            turn_data=turn_data,
        )

    def test_get_geval_config_returns_none_when_not_found(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that None is returned when MetricManager finds no config."""
        mock_metric_manager.get_metric_metadata.return_value = None

        conv_data = mocker.MagicMock()
        config = handler._get_geval_config(
            metric_name="nonexistent_metric",
            conv_data=conv_data,
            turn_data=None,
            is_conversation=True,
        )

        assert config is None

    def test_evaluate_missing_config(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that evaluate returns error when config is not found."""
        mock_metric_manager.get_metric_metadata.return_value = None

        conv_data = mocker.MagicMock()
        score, reason = handler.evaluate(
            metric_name="nonexistent",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=None,
            is_conversation=True,
        )

        assert score is None
        assert "configuration not found" in reason.lower()

    def test_evaluate_missing_criteria(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that evaluate requires 'criteria' in config."""
        mock_metric_manager.get_metric_metadata.return_value = {
            "threshold": 0.8,
            "evaluation_params": ["query", "response"],
            # Missing 'criteria'
        }

        conv_data = mocker.MagicMock()
        score, reason = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=None,
            is_conversation=True,
        )

        assert score is None
        assert "criteria" in reason.lower()

    def test_evaluate_turn_missing_turn_data(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that turn-level evaluation requires turn_data."""
        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Test criteria"
        }

        conv_data = mocker.MagicMock()
        score, reason = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=None,  # Missing required turn data
            is_conversation=False,  # Turn-level
        )

        assert score is None
        assert "turn data required" in reason.lower()

    def test_evaluate_turn_success(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test successful turn-level evaluation."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        # Mock GEval metric instance
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.85
        mock_metric.reason = "Test passed"
        mock_geval_class.return_value = mock_metric

        # Setup metric manager to return config
        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Test criteria",
            "evaluation_params": ["query", "response"],
            "evaluation_steps": ["Step 1", "Step 2"],
            "threshold": 0.7,
        }

        # Mock turn data
        turn_data = mocker.MagicMock()
        turn_data.query = "Test query"
        turn_data.response = "Test response"
        turn_data.expected_response = None
        turn_data.contexts = None

        conv_data = mocker.MagicMock()

        score, reason = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )

        assert score == 0.85
        assert reason == "Test passed"
        mock_metric.measure.assert_called_once()

    def test_evaluate_turn_with_optional_fields(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test turn-level evaluation includes optional fields when present."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_test_case_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.LLMTestCase"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.75
        mock_metric.reason = "Good match"
        mock_geval_class.return_value = mock_metric

        mock_test_case = mocker.MagicMock()
        mock_test_case_class.return_value = mock_test_case

        # Setup metric manager
        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Compare against expected",
            "evaluation_params": ["query", "response", "expected_response"],
            "threshold": 0.7,
        }

        # Mock turn data with all optional fields
        turn_data = mocker.MagicMock()
        turn_data.query = "Test query"
        turn_data.response = "Test response"
        turn_data.expected_response = "Expected response"
        turn_data.contexts = ["Context 1", "Context 2"]

        conv_data = mocker.MagicMock()

        handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )

        # Verify test case was created with optional fields
        call_kwargs = mock_test_case_class.call_args[1]
        assert call_kwargs["input"] == "Test query"
        assert call_kwargs["actual_output"] == "Test response"
        assert call_kwargs["expected_output"] == "Expected response"
        assert call_kwargs["context"] == ["Context 1", "Context 2"]

    def test_evaluate_turn_none_score_returns_zero(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that None score from metric is converted to 0.0."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = None
        mock_metric.reason = "Could not evaluate"
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Test criteria",
            "threshold": 0.7,
        }

        turn_data = mocker.MagicMock()
        turn_data.query = "Test query"
        turn_data.response = "Test response"
        turn_data.expected_response = None
        turn_data.contexts = None

        conv_data = mocker.MagicMock()

        score, reason = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )

        # Should return 0.0 when score is None
        assert score == 0.0
        assert reason == "Could not evaluate"

    def test_evaluate_turn_handles_exceptions(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that turn evaluation handles exceptions gracefully."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.measure.side_effect = ValueError("Test error")
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Test criteria",
            "threshold": 0.7,
        }

        turn_data = mocker.MagicMock()
        turn_data.query = "Test query"
        turn_data.response = "Test response"
        turn_data.expected_response = None
        turn_data.contexts = None

        conv_data = mocker.MagicMock()

        score, reason = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )

        assert score is None
        assert "evaluation error" in reason.lower()
        assert "Test error" in reason

    def test_evaluate_turn_uses_default_params_when_none_provided(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that default evaluation_params are used when none provided."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.8
        mock_metric.reason = "Good"
        mock_geval_class.return_value = mock_metric

        # Config with no evaluation_params
        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Test criteria",
            "threshold": 0.7,
        }

        turn_data = mocker.MagicMock()
        turn_data.query = "Test query"
        turn_data.response = "Test response"
        turn_data.expected_response = None
        turn_data.contexts = None

        conv_data = mocker.MagicMock()

        handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )

        # Verify GEval was called with default params
        call_kwargs = mock_geval_class.call_args[1]
        assert call_kwargs["evaluation_params"] == [
            LLMTestCaseParams.INPUT,
            LLMTestCaseParams.ACTUAL_OUTPUT,
        ]

    def test_evaluate_conversation_success(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test successful conversation-level evaluation."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.90
        mock_metric.reason = "Conversation coherent"
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Conversation criteria",
            "evaluation_params": ["query", "response"],
            "threshold": 0.6,
        }

        # Mock conversation data with multiple turns
        turn1 = mocker.MagicMock()
        turn1.query = "Query 1"
        turn1.response = "Response 1"

        turn2 = mocker.MagicMock()
        turn2.query = "Query 2"
        turn2.response = "Response 2"

        conv_data = mocker.MagicMock()
        conv_data.turns = [turn1, turn2]

        score, reason = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=None,
            turn_data=None,
            is_conversation=True,
        )

        assert score == 0.90
        assert reason == "Conversation coherent"
        mock_metric.measure.assert_called_once()

    def test_evaluate_conversation_aggregates_turns(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that conversation evaluation properly aggregates turn data."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_test_case_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.LLMTestCase"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.85
        mock_metric.reason = "Good conversation"
        mock_geval_class.return_value = mock_metric

        mock_test_case = mocker.MagicMock()
        mock_test_case_class.return_value = mock_test_case

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Conversation flow",
            "threshold": 0.7,
        }

        # Create multiple turns including one with None response
        turn1 = mocker.MagicMock()
        turn1.query = "First question"
        turn1.response = "First answer"

        turn2 = mocker.MagicMock()
        turn2.query = "Second question"
        turn2.response = "Second answer"

        turn3 = mocker.MagicMock()
        turn3.query = "Third question"
        turn3.response = None  # Test None response handling

        conv_data = mocker.MagicMock()
        conv_data.turns = [turn1, turn2, turn3]

        handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=None,
            turn_data=None,
            is_conversation=True,
        )

        # Verify test case was created with aggregated input/output
        call_kwargs = mock_test_case_class.call_args[1]
        assert "Turn 1 - User: First question" in call_kwargs["input"]
        assert "Turn 2 - User: Second question" in call_kwargs["input"]
        assert "Turn 3 - User: Third question" in call_kwargs["input"]
        assert "Turn 1 - Assistant: First answer" in call_kwargs["actual_output"]
        assert "Turn 2 - Assistant: Second answer" in call_kwargs["actual_output"]
        assert "Turn 3 - Assistant:" in call_kwargs["actual_output"]

    def test_evaluate_conversation_with_evaluation_steps(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that evaluation_steps are passed to GEval when provided."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.88
        mock_metric.reason = "Follows steps"
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Multi-step evaluation",
            "evaluation_params": ["query", "response"],
            "evaluation_steps": [
                "Check coherence",
                "Verify context",
                "Assess relevance",
            ],
            "threshold": 0.7,
        }

        turn1 = mocker.MagicMock()
        turn1.query = "Query 1"
        turn1.response = "Response 1"

        conv_data = mocker.MagicMock()
        conv_data.turns = [turn1]

        handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=None,
            turn_data=None,
            is_conversation=True,
        )

        # Verify evaluation_steps were passed to GEval
        call_kwargs = mock_geval_class.call_args[1]
        assert call_kwargs["evaluation_steps"] == [
            "Check coherence",
            "Verify context",
            "Assess relevance",
        ]

    def test_evaluate_conversation_handles_exceptions(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that conversation evaluation handles exceptions gracefully."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.measure.side_effect = RuntimeError("API error")
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Test criteria",
            "threshold": 0.7,
        }

        turn1 = mocker.MagicMock()
        turn1.query = "Query 1"
        turn1.response = "Response 1"

        conv_data = mocker.MagicMock()
        conv_data.turns = [turn1]

        score, reason = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=None,
            turn_data=None,
            is_conversation=True,
        )

        assert score is None
        assert "evaluation error" in reason.lower()
        assert "API error" in reason

    def test_evaluate_turn_with_rubrics_passes_rubric_to_geval(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that turn-level evaluation passes rubrics to GEval."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.8
        mock_metric.reason = "Good"
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Test criteria",
            "evaluation_params": ["query", "response"],
            "threshold": 0.5,
            "rubrics": [
                {"score_range": [0, 3], "expected_outcome": "Poor"},
                {"score_range": [4, 7], "expected_outcome": "Good"},
                {"score_range": [8, 10], "expected_outcome": "Excellent"},
            ],
        }

        turn_data = mocker.MagicMock()
        turn_data.query = "Q"
        turn_data.response = "R"
        turn_data.expected_response = None
        turn_data.contexts = None
        conv_data = mocker.MagicMock()

        handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )

        call_kwargs = mock_geval_class.call_args[1]
        assert "rubric" in call_kwargs
        rubric_list = call_kwargs["rubric"]
        assert len(rubric_list) == 3
        assert all(isinstance(r, Rubric) for r in rubric_list)
        assert rubric_list[0].score_range == (0, 3)
        assert rubric_list[1].expected_outcome == "Good"

    def test_evaluate_conversation_with_rubrics_passes_rubric_to_geval(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that conversation-level evaluation passes rubrics to GEval."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.75
        mock_metric.reason = "Coherent"
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Coherence",
            "threshold": 0.6,
            "rubrics": [
                {"score_range": [0, 4], "expected_outcome": "Weak"},
                {"score_range": [5, 10], "expected_outcome": "Strong"},
            ],
        }

        turn1 = mocker.MagicMock()
        turn1.query = "Q1"
        turn1.response = "R1"
        conv_data = mocker.MagicMock()
        conv_data.turns = [turn1]

        handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=None,
            turn_data=None,
            is_conversation=True,
        )

        call_kwargs = mock_geval_class.call_args[1]
        assert "rubric" in call_kwargs
        assert len(call_kwargs["rubric"]) == 2

    def test_evaluate_with_both_criteria_and_rubrics_passes_both(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that when both criteria and rubrics are present, both are passed to GEval."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.9
        mock_metric.reason = "OK"
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Correctness criteria",
            "evaluation_steps": ["Step one", "Step two"],
            "threshold": 0.7,
            "rubrics": [
                {"score_range": [0, 5], "expected_outcome": "Low"},
                {"score_range": [6, 10], "expected_outcome": "High"},
            ],
        }

        turn_data = mocker.MagicMock()
        turn_data.query = "Q"
        turn_data.response = "R"
        turn_data.expected_response = None
        turn_data.contexts = None
        conv_data = mocker.MagicMock()

        handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )

        call_kwargs = mock_geval_class.call_args[1]
        assert call_kwargs["criteria"] == "Correctness criteria"
        assert call_kwargs["evaluation_steps"] == ["Step one", "Step two"]
        assert "rubric" in call_kwargs
        assert len(call_kwargs["rubric"]) == 2

    def test_evaluate_with_invalid_rubrics_structure_returns_error(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that invalid rubric structure (e.g. missing expected_outcome) returns error."""
        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Some criteria",
            "threshold": 0.5,
            "rubrics": [{"score_range": [0, 5]}],  # missing expected_outcome
        }
        turn_data = mocker.MagicMock()
        turn_data.query = "Q"
        turn_data.response = "R"
        turn_data.expected_response = None
        turn_data.contexts = None
        conv_data = mocker.MagicMock()

        score, reason = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )

        assert score is None
        assert "expected_outcome" in reason

    def test_evaluate_turn_score_passed_through(
        self,
        handler: GEvalHandler,
        mock_metric_manager: Any,
        mocker: MockerFixture,
    ) -> None:
        """Test that GEval score is passed through as-is (DeepEval normalizes to 0-1)."""
        mock_geval_class = mocker.patch(
            "lightspeed_evaluation.core.metrics.geval.GEval"
        )
        mock_metric = mocker.MagicMock()
        mock_metric.score = 0.85
        mock_metric.reason = "OK"
        mock_geval_class.return_value = mock_metric

        mock_metric_manager.get_metric_metadata.return_value = {
            "criteria": "Test",
            "threshold": 0.5,
        }
        turn_data = mocker.MagicMock()
        turn_data.query = "Q"
        turn_data.response = "R"
        turn_data.expected_response = None
        turn_data.contexts = None
        conv_data = mocker.MagicMock()

        score, _ = handler.evaluate(
            metric_name="test_metric",
            conv_data=conv_data,
            _turn_idx=0,
            turn_data=turn_data,
            is_conversation=False,
        )
        assert score == 0.85
